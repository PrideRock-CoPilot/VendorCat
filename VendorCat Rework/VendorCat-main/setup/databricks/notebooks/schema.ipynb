{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a051143-7a49-448b-9b68-e194ef6beb5e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialization and Parameter Setup"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4789367278503194>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Databricks notebook initialization for schema bootstrap\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Set up widgets for catalog, schema, and environment\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m dbx_catalog \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcatalog\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcatalog\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m dbx_schema \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      6\u001B[0m dbx_env \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/WidgetHandlerImpl.py:82\u001B[0m, in \u001B[0;36mWidgetsHandlerImpl.get\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Returns the current value of a widget with the given name.\u001B[39;00m\n",
       "\u001B[1;32m     44\u001B[0m \n",
       "\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m    :param name: Name of the argument to be accessed\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     80\u001B[0m \u001B[38;5;124;03m        ```\u001B[39;00m\n",
       "\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 82\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_notebookArguments\u001B[38;5;241m.\u001B[39mgetArgument(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_entry_point\u001B[38;5;241m.\u001B[39mgetCurrentBindings())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o2.getArgument.\n",
       ": com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named catalog is defined\n",
       "\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:74)\n",
       "\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:260)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o2.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named catalog is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:74)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:260)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o2.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named catalog is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:74)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:260)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-4789367278503194>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Databricks notebook initialization for schema bootstrap\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Set up widgets for catalog, schema, and environment\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m dbx_catalog \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcatalog\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcatalog\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m dbx_schema \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      6\u001B[0m dbx_env \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m dbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/WidgetHandlerImpl.py:82\u001B[0m, in \u001B[0;36mWidgetsHandlerImpl.get\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     43\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Returns the current value of a widget with the given name.\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m    :param name: Name of the argument to be accessed\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;124;03m        ```\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 82\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_notebookArguments\u001B[38;5;241m.\u001B[39mgetArgument(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_entry_point\u001B[38;5;241m.\u001B[39mgetCurrentBindings())\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o2.getArgument.\n: com.databricks.dbutils_v1.InputWidgetNotDefined: No input widget named catalog is defined\n\tat com.databricks.backend.daemon.driver.NotebookArguments.checkExists(NotebookArguments.scala:74)\n\tat com.databricks.backend.daemon.driver.NotebookArguments.getArgument(NotebookArguments.scala:260)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Databricks notebook initialization for schema bootstrap\n",
    "# Set up widgets for catalog, schema, and environment\n",
    "\n",
    "dbx_catalog = dbutils.widgets.get(\"catalog\") if dbutils.widgets.get(\"catalog\") else \"main\"\n",
    "dbx_schema = dbutils.widgets.get(\"schema\") if dbutils.widgets.get(\"schema\") else \"v1\"\n",
    "dbx_env = dbutils.widgets.get(\"environment\") if dbutils.widgets.get(\"environment\") else \"dev\"\n",
    "\n",
    "# Switch context to specified catalog and schema\n",
    "spark.sql(f\"USE CATALOG {dbx_catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {dbx_schema}\")\n",
    "\n",
    "print(f\"Schema bootstrap starting for catalog: {dbx_catalog}, schema: {dbx_schema}, environment: {dbx_env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2287275-0d9e-4e52-80e4-9c329af849c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create lkp_line_of_business Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create lkp_line_of_business lookup table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lkp_line_of_business (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    code STRING,\n",
    "    name STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    sort_order INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"lkp_line_of_business table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac0c7bb1-f24b-41be-8875-7c83d225ad21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create lkp_service_type Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create lkp_service_type lookup table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lkp_service_type (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    code STRING,\n",
    "    name STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    sort_order INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"lkp_service_type table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c5f5d7a-65af-4854-bd24-c7660e4ec45e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create lkp_owner_role Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create lkp_owner_role lookup table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lkp_owner_role (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    code STRING,\n",
    "    name STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    sort_order INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"lkp_owner_role table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2467b085-3fc2-46f9-aaeb-251c3c2d1647",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create lkp_contact_type Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create lkp_contact_type lookup table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lkp_contact_type (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    code STRING,\n",
    "    name STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    sort_order INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"lkp_contact_type table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62aeab62-f285-48b3-89d8-6abecab9dc53",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create lkp_lifecycle_state Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create lkp_lifecycle_state lookup table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lkp_lifecycle_state (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    code STRING,\n",
    "    name STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    sort_order INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"lkp_lifecycle_state table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e97b3c11-77d0-4bfd-914f-c6d930015786",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create lkp_risk_tier Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create lkp_risk_tier lookup table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lkp_risk_tier (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    code STRING,\n",
    "    name STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    sort_order INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"lkp_risk_tier table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e221a24e-019f-455a-b2f3-0673e4090f4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor core business table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor (\n",
    "    vendor_id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    name STRING,\n",
    "    status STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb4fd03-8fbd-4641-a55b-67f39c190f8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create offering Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create offering core business table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS offering (\n",
    "    offering_id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    vendor_id BIGINT,\n",
    "    name STRING,\n",
    "    status STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"offering table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd1d4b3-39f6-48a6-8ba3-a425487b254e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor_identifier Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor_identifier table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor_identifier (\n",
    "    vendor_identifier_id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    vendor_id BIGINT,\n",
    "    identifier_type STRING,\n",
    "    identifier_value STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor_identifier table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1efecd8d-7761-4acc-997a-9e9f7d7624d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create project Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create project core business table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS project (\n",
    "    project_id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    name STRING,\n",
    "    status STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"project table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4000d313-de41-43b9-b63e-6d5a71f5fe67",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create project_offering_map Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create project_offering_map table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS project_offering_map (\n",
    "    project_id BIGINT,\n",
    "    offering_id BIGINT,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"project_offering_map table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c5a08d-bd21-4e29-ba9c-9cf08a7920f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor_lob_assignment Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor_lob_assignment table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor_lob_assignment (\n",
    "    vendor_id BIGINT,\n",
    "    line_of_business_id BIGINT,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor_lob_assignment table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade4b13b-bf88-458f-931d-d5ff2f808c71",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create offering_lob_assignment Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create offering_lob_assignment table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS offering_lob_assignment (\n",
    "    offering_id BIGINT,\n",
    "    line_of_business_id BIGINT,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"offering_lob_assignment table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f92259-edb0-4074-b1f1-6f6fcad5ab73",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor_owner_assignment Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor_owner_assignment table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor_owner_assignment (\n",
    "    vendor_id BIGINT,\n",
    "    owner_role_id BIGINT,\n",
    "    user_principal STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor_owner_assignment table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69632ab4-4416-4ace-a86c-3ca07e681cd7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create offering_owner_assignment Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create offering_owner_assignment table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS offering_owner_assignment (\n",
    "    offering_id BIGINT,\n",
    "    owner_role_id BIGINT,\n",
    "    user_principal STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"offering_owner_assignment table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c17421-47d7-467f-b4cf-61d4f72a3e59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create project_owner_assignment Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create project_owner_assignment table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS project_owner_assignment (\n",
    "    project_id BIGINT,\n",
    "    owner_role_id BIGINT,\n",
    "    user_principal STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"project_owner_assignment table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "539b035d-2252-4f1c-a141-d09912338425",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor_contact Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor_contact table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor_contact (\n",
    "    vendor_id BIGINT,\n",
    "    contact_type_id BIGINT,\n",
    "    contact_value STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor_contact table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf3294aa-287b-475a-92da-7852779a9a80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create offering_contact Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create offering_contact table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS offering_contact (\n",
    "    offering_id BIGINT,\n",
    "    contact_type_id BIGINT,\n",
    "    contact_value STRING,\n",
    "    is_active BOOLEAN,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"offering_contact table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c372ddf5-1258-456f-b6db-76ce0fbc66f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create change_request Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create change_request table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS change_request (\n",
    "    change_request_id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    entity_type STRING,\n",
    "    entity_id BIGINT,\n",
    "    request_type STRING,\n",
    "    status STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"change_request table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc939f8a-1452-43bc-bfb6-28c56d4833f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor_merge_event Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor_merge_event table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor_merge_event (\n",
    "    merge_event_id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    survivor_vendor_id BIGINT,\n",
    "    merged_vendor_id BIGINT,\n",
    "    merge_type STRING,\n",
    "    status STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor_merge_event table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b542e44-be39-461f-8d33-7454acb542f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor_merge_member Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor_merge_member table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor_merge_member (\n",
    "    merge_event_id BIGINT,\n",
    "    vendor_id BIGINT,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor_merge_member table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d47dbb9b-1ac0-4bea-b606-5e1804077466",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor_merge_snapshot Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor_merge_snapshot table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor_merge_snapshot (\n",
    "    merge_event_id BIGINT,\n",
    "    snapshot_data STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor_merge_snapshot table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5643322-e8fd-47f6-a9c7-59d789071410",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create vendor_survivorship_decision Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create vendor_survivorship_decision table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS vendor_survivorship_decision (\n",
    "    merge_event_id BIGINT,\n",
    "    decision_data STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"vendor_survivorship_decision table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "840914cc-0cb1-4433-ae25-b6fb3f665616",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create change_event Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create change_event table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS change_event (\n",
    "    change_event_id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    entity_type STRING,\n",
    "    entity_id BIGINT,\n",
    "    event_type STRING,\n",
    "    event_data STRING,\n",
    "    created_at TIMESTAMP,\n",
    "    updated_at TIMESTAMP,\n",
    "    audit_user STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"change_event table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e3a3c33-7f2d-4b6c-8cbc-04cf169da2b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create schema_version Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create schema_version table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS schema_version (\n",
    "    version STRING,\n",
    "    applied_at TIMESTAMP,\n",
    "    applied_by STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "print(\"schema_version table created.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "schema",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}