{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "v1_apply_db_changes_md_01",
                "language": "markdown"
            },
            "source": [
                "# V1 Database Changes (Databricks)\n",
                "\n",
                "Applies schema/runtime deltas that are newer than the legacy schema notebook.\n",
                "This includes additional runtime tables, reporting view updates, and optimize statements."
            ],
            "id": "v1_apply_db_changes_md_01"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "v1_apply_db_changes_code_01",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "dbutils.widgets.text('catalog', 'vendorcat_dev')\n",
                "dbutils.widgets.text('schema', 'vendorcat_v1')\n",
                "dbutils.widgets.text('sql_root', '/Workspace/Repos/PrideRock-CoPilot/VendorCat/setup/v1_schema/databricks')\n",
                "\n",
                "catalog = dbutils.widgets.get('catalog').strip()\n",
                "schema = dbutils.widgets.get('schema').strip()\n",
                "sql_root = dbutils.widgets.get('sql_root').strip()\n",
                "\n",
                "assert catalog, 'catalog parameter is required'\n",
                "assert schema, 'schema parameter is required'\n",
                "assert sql_root, 'sql_root parameter is required'\n",
                "\n",
                "spark.sql(f\"USE CATALOG `{catalog}`\")\n",
                "spark.sql(f\"USE SCHEMA `{schema}`\")\n",
                "print(f'Applying database changes for catalog={catalog} schema={schema}')\n",
                "print(f'SQL root: {sql_root}')"
            ],
            "id": "v1_apply_db_changes_code_01"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "v1_apply_db_changes_code_02",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import re\n",
                "from pathlib import Path\n",
                "\n",
                "token_pattern = re.compile(r'\\$\\{(CATALOG|SCHEMA)\\}')\n",
                "\n",
                "def render_sql(sql_text: str, catalog_name: str, schema_name: str) -> str:\n",
                "    context = {'CATALOG': catalog_name, 'SCHEMA': schema_name}\n",
                "    return token_pattern.sub(lambda m: context[m.group(1)], sql_text)\n",
                "\n",
                "def execute_sql_script(file_path: str) -> None:\n",
                "    path = Path(file_path)\n",
                "    if not path.exists():\n",
                "        raise FileNotFoundError(f'SQL file not found: {file_path}')\n",
                "    raw = path.read_text(encoding='utf-8')\n",
                "    rendered = render_sql(raw, catalog, schema)\n",
                "    statements = [stmt.strip() for stmt in rendered.split(';') if stmt.strip()]\n",
                "    for statement in statements:\n",
                "        spark.sql(statement)\n",
                "    print(f'Applied {path.name} ({len(statements)} statements)')"
            ],
            "id": "v1_apply_db_changes_code_02"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "v1_apply_db_changes_code_03",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "schema_files = [\n",
                "    f'{sql_root}/05_create_functional_parity_bridge.sql',\n",
                "    f'{sql_root}/06_create_functional_runtime_compat.sql',\n",
                "    f'{sql_root}/07_create_reporting_views.sql',\n",
                "    f'{sql_root}/90_create_indexes.sql',\n",
                "]\n",
                "\n",
                "for sql_file in schema_files:\n",
                "    execute_sql_script(sql_file)\n",
                "\n",
                "print('Database changes applied.')"
            ],
            "id": "v1_apply_db_changes_code_03"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "v1_apply_db_changes_code_04",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "required_tables = [\n",
                "    'app_vendor_warning',\n",
                "    'app_offering_payment',\n",
                "    'app_import_job',\n",
                "    'app_import_stage_row',\n",
                "    'app_import_stage_payment',\n",
                "]\n",
                "\n",
                "for table_name in required_tables:\n",
                "    exists = spark.sql(\n",
                "        f\"SHOW TABLES IN `{catalog}`.`{schema}` LIKE '{table_name}'\"\n",
                "    ).count() > 0\n",
                "    status = 'OK' if exists else 'MISSING'\n",
                "    print(f\"{table_name}: {status}\")\n",
                "\n",
                "view_exists = spark.sql(\n",
                "    f\"SHOW VIEWS IN `{catalog}`.`{schema}` LIKE 'vw_employee_directory'\"\n",
                ").count() > 0\n",
                "view_status = 'OK' if view_exists else 'MISSING'\n",
                "print(f\"vw_employee_directory view: {view_status}\")"
            ],
            "id": "v1_apply_db_changes_code_04"
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
