{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prod_push_md_01",
    "language": "markdown"
   },
   "source": [
    "# Production Push Setup\n",
    "\n",
    "This notebook orchestrates production push operations for the Vendor Catalog schema bundle.\n",
    "\n",
    "Modes:\n",
    "1. `apply`: apply object SQL, optional base seed SQL, then validate.\n",
    "2. `drop_except`: drop all views/tables except excluded object names.\n",
    "3. `truncate_except`: truncate all tables except excluded object names.\n",
    "4. `rebuild`: preflight write permissions, drop non-excluded objects, then apply+seed+validate.\n",
    "5. `validate`: validate expected tables/views from SQL object scripts.\n",
    "\n",
    "Destructive modes are `drop_except`, `truncate_except`, and `rebuild`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prod_push_code_01",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"a1_dlk\")\n",
    "dbutils.widgets.text(\"schema\", \"twvendor\")\n",
    "dbutils.widgets.text(\"sql_root\", \"/Workspace/Repos/<repo>/setup/production_push/sql\")\n",
    "dbutils.widgets.dropdown(\"operation\", \"apply\", [\"apply\", \"drop_except\", \"truncate_except\", \"rebuild\", \"validate\"])\n",
    "dbutils.widgets.text(\"exclude_objects\", \"schema_version\")\n",
    "dbutils.widgets.dropdown(\"include_seed\", \"true\", [\"true\", \"false\"])\n",
    "dbutils.widgets.dropdown(\"include_optimize\", \"true\", [\"true\", \"false\"])\n",
    "dbutils.widgets.dropdown(\"dry_run\", \"false\", [\"true\", \"false\"])\n",
    "dbutils.widgets.dropdown(\"confirm_destructive\", \"false\", [\"false\", \"true\"])\n",
    "\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "\n",
    "\n",
    "def parse_bool(value: str) -> bool:\n",
    "    return str(value or \"\").strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"on\"}\n",
    "\n",
    "\n",
    "def resolve_sql_root(raw_value: str) -> str:\n",
    "    value = str(raw_value or \"\").strip()\n",
    "    if \"<repo>\" not in value:\n",
    "        return value\n",
    "    try:\n",
    "        notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        notebook_path = str(notebook_path or \"\").strip()\n",
    "        if not notebook_path:\n",
    "            return value\n",
    "        base_dir = notebook_path.rsplit(\"/\", 1)[0]\n",
    "        if base_dir.startswith(\"/Workspace/\"):\n",
    "            candidate = f\"{base_dir}/sql\"\n",
    "        elif base_dir.startswith(\"/\"):\n",
    "            candidate = f\"/Workspace{base_dir}/sql\"\n",
    "        else:\n",
    "            candidate = f\"/Workspace/{base_dir}/sql\"\n",
    "        return candidate\n",
    "    except Exception:\n",
    "        return value\n",
    "\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\").strip()\n",
    "schema = dbutils.widgets.get(\"schema\").strip()\n",
    "raw_sql_root = dbutils.widgets.get(\"sql_root\").strip()\n",
    "sql_root = resolve_sql_root(raw_sql_root)\n",
    "operation = dbutils.widgets.get(\"operation\").strip().lower()\n",
    "exclude_objects = dbutils.widgets.get(\"exclude_objects\").strip()\n",
    "include_seed = parse_bool(dbutils.widgets.get(\"include_seed\"))\n",
    "include_optimize = parse_bool(dbutils.widgets.get(\"include_optimize\"))\n",
    "dry_run = parse_bool(dbutils.widgets.get(\"dry_run\"))\n",
    "confirm_destructive = parse_bool(dbutils.widgets.get(\"confirm_destructive\"))\n",
    "\n",
    "assert catalog, \"catalog is required\"\n",
    "assert schema, \"schema is required\"\n",
    "assert sql_root, \"sql_root is required\"\n",
    "assert operation in {\"apply\", \"drop_except\", \"truncate_except\", \"rebuild\", \"validate\"}, \"invalid operation\"\n",
    "\n",
    "if operation in {\"drop_except\", \"truncate_except\", \"rebuild\"} and not confirm_destructive and not dry_run:\n",
    "    raise AssertionError(\n",
    "        \"Destructive operation blocked. Set confirm_destructive=true (or dry_run=true).\"\n",
    "    )\n",
    "\n",
    "if operation in {\"apply\", \"rebuild\"} and dry_run:\n",
    "    print(\"WARNING: dry_run=true means no tables, views, or seed data will be created.\")\n",
    "\n",
    "run_id = uuid.uuid4().hex[:12]\n",
    "run_ts_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "print(f\"run_id={run_id} run_ts_utc={run_ts_utc}\")\n",
    "print(f\"catalog={catalog} schema={schema}\")\n",
    "print(\n",
    "    f\"operation={operation} include_seed={include_seed} include_optimize={include_optimize} \"\n",
    "    f\"dry_run={dry_run} confirm_destructive={confirm_destructive}\"\n",
    ")\n",
    "if raw_sql_root != sql_root:\n",
    "    print(f\"sql_root resolved from placeholder: {raw_sql_root} -> {sql_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prod_push_code_02",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "TOKEN_PATTERN = re.compile(r\"\\$\\{(CATALOG|SCHEMA)\\}\")\n",
    "\n",
    "execution_stats = {\n",
    "    \"files\": [],\n",
    "    \"statements_executed\": 0,\n",
    "    \"statements_previewed\": 0,\n",
    "    \"statements_skipped\": 0,\n",
    "    \"statements_failed\": 0,\n",
    "    \"retries\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "def render_tokens(sql_text: str) -> str:\n",
    "    context = {\"CATALOG\": catalog, \"SCHEMA\": schema}\n",
    "    return TOKEN_PATTERN.sub(lambda m: context[m.group(1)], sql_text)\n",
    "\n",
    "\n",
    "def split_sql_statements(sql_text: str) -> list[str]:\n",
    "    statements: list[str] = []\n",
    "    current: list[str] = []\n",
    "    in_single = False\n",
    "    in_double = False\n",
    "    idx = 0\n",
    "\n",
    "    while idx < len(sql_text):\n",
    "        ch = sql_text[idx]\n",
    "        nxt = sql_text[idx + 1] if idx + 1 < len(sql_text) else \"\"\n",
    "\n",
    "        if ch == \"'\" and not in_double:\n",
    "            current.append(ch)\n",
    "            if in_single and nxt == \"'\":\n",
    "                current.append(nxt)\n",
    "                idx += 1\n",
    "            else:\n",
    "                in_single = not in_single\n",
    "        elif ch == '\"' and not in_single:\n",
    "            in_double = not in_double\n",
    "            current.append(ch)\n",
    "        elif ch == \";\" and not in_single and not in_double:\n",
    "            stmt = \"\".join(current).strip()\n",
    "            if stmt:\n",
    "                statements.append(stmt)\n",
    "            current = []\n",
    "        else:\n",
    "            current.append(ch)\n",
    "        idx += 1\n",
    "\n",
    "    tail = \"\".join(current).strip()\n",
    "    if tail:\n",
    "        statements.append(tail)\n",
    "\n",
    "    return statements\n",
    "\n",
    "\n",
    "def normalize_object_name(token: str) -> str:\n",
    "    value = str(token or \"\").strip().replace(\"`\", \"\")\n",
    "    if \".\" in value:\n",
    "        value = value.split(\".\")[-1]\n",
    "    return value.strip().lower()\n",
    "\n",
    "\n",
    "def with_default_feature(statement: str) -> str:\n",
    "    if \"USING DELTA\" in statement.upper() and \"TBLPROPERTIES\" not in statement.upper():\n",
    "        return re.sub(\n",
    "            r\"(?i)USING\\s+DELTA\",\n",
    "            \"USING DELTA TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported')\",\n",
    "            statement,\n",
    "            count=1,\n",
    "        )\n",
    "    return statement\n",
    "\n",
    "\n",
    "def execute_sql(statement: str) -> bool:\n",
    "    if dry_run:\n",
    "        preview = statement if len(statement) <= 180 else (statement[:180] + \"...\")\n",
    "        print(f\"[dry_run] {preview}\")\n",
    "        return False\n",
    "    spark.sql(statement)\n",
    "    return True\n",
    "\n",
    "\n",
    "def has_permission_error(message: str) -> bool:\n",
    "    lowered = str(message or \"\").lower()\n",
    "    return (\n",
    "        \"insufficient privileges\" in lowered\n",
    "        or \"insufficient_permissions\" in lowered\n",
    "        or \"permission_denied\" in lowered\n",
    "        or \"not authorized\" in lowered\n",
    "    )\n",
    "\n",
    "\n",
    "def catalog_and_schema_are_usable() -> bool:\n",
    "    try:\n",
    "        spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "        spark.sql(f\"USE SCHEMA `{schema}`\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def catalog_is_usable() -> bool:\n",
    "    try:\n",
    "        spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def execute_sql_statement(statement: str, *, file_name: str, index: int) -> str:\n",
    "    try:\n",
    "        ran = execute_sql(statement)\n",
    "        if ran:\n",
    "            execution_stats[\"statements_executed\"] += 1\n",
    "            return \"executed\"\n",
    "        execution_stats[\"statements_previewed\"] += 1\n",
    "        return \"previewed\"\n",
    "    except Exception as exc:\n",
    "        message = str(exc)\n",
    "        upper = statement.upper()\n",
    "\n",
    "        if upper.startswith(\"CREATE CATALOG IF NOT EXISTS\") and has_permission_error(message):\n",
    "            if catalog_is_usable():\n",
    "                print(f\"[skip-create-catalog-no-privilege] {file_name}#{index}\")\n",
    "                execution_stats[\"statements_skipped\"] += 1\n",
    "                return \"skipped\"\n",
    "\n",
    "        if upper.startswith(\"CREATE SCHEMA IF NOT EXISTS\") and has_permission_error(message):\n",
    "            if catalog_and_schema_are_usable():\n",
    "                print(f\"[skip-create-schema-no-privilege] {file_name}#{index}\")\n",
    "                execution_stats[\"statements_skipped\"] += 1\n",
    "                return \"skipped\"\n",
    "\n",
    "        if upper.startswith(\"OPTIMIZE \") and (\"TABLE_OR_VIEW_NOT_FOUND\" in message or \"not found\" in message.lower()):\n",
    "            print(f\"[skip-optimize-missing] {file_name}#{index}\")\n",
    "            execution_stats[\"statements_skipped\"] += 1\n",
    "            return \"skipped\"\n",
    "\n",
    "        if upper.startswith(\"CREATE TABLE\") and \"DEFAULT\" in upper and \"WRONG_COLUMN_DEFAULTS_FOR_DELTA_FEATURE_NOT_ENABLED\" in message:\n",
    "            patched = with_default_feature(statement)\n",
    "            print(f\"[retry-default-feature] {file_name}#{index}\")\n",
    "            execution_stats[\"retries\"] += 1\n",
    "            ran = execute_sql(patched)\n",
    "            if ran:\n",
    "                execution_stats[\"statements_executed\"] += 1\n",
    "                return \"executed\"\n",
    "            execution_stats[\"statements_previewed\"] += 1\n",
    "            return \"previewed\"\n",
    "\n",
    "        if \"CREATE OR REPLACE VIEW VW_EMPLOYEE_DIRECTORY\" in upper and (\n",
    "            \"TABLE_OR_VIEW_NOT_FOUND\" in message\n",
    "            or \"PERMISSION_DENIED\" in message\n",
    "            or \"insufficient privileges\" in message.lower()\n",
    "        ):\n",
    "            fallback = f\"\"\"\n",
    "            CREATE OR REPLACE VIEW `{catalog}`.`{schema}`.vw_employee_directory AS\n",
    "            SELECT\n",
    "              employee_id,\n",
    "              lower(coalesce(network_id, login_identifier, email)) AS login_identifier,\n",
    "              network_id,\n",
    "              email,\n",
    "              first_name,\n",
    "              last_name,\n",
    "              display_name,\n",
    "              CASE\n",
    "                WHEN lower(trim(cast(active_flag AS STRING))) IN ('1', 'true', 't', 'y', 'yes', 'a', 'active') THEN 'A'\n",
    "                ELSE 'T'\n",
    "              END AS active_flag,\n",
    "              CASE\n",
    "                WHEN lower(trim(cast(active_flag AS STRING))) IN ('1', 'true', 't', 'y', 'yes', 'a', 'active') THEN 'Active'\n",
    "                ELSE 'Inactive'\n",
    "              END AS employment_status,\n",
    "              NULL AS last_action_date,\n",
    "              NULL AS hire_date,\n",
    "              NULL AS termination_date,\n",
    "              NULL AS job_title,\n",
    "              NULL AS department_name,\n",
    "              NULL AS manager_level,\n",
    "              'Unknown' AS hierarchy_tier,\n",
    "              1 AS default_security_level,\n",
    "              manager_id\n",
    "            FROM `{catalog}`.`{schema}`.app_employee_directory\n",
    "            \"\"\".strip()\n",
    "            print(f\"[fallback-vw_employee_directory] {file_name}#{index}\")\n",
    "            execution_stats[\"retries\"] += 1\n",
    "            ran = execute_sql(fallback)\n",
    "            if ran:\n",
    "                execution_stats[\"statements_executed\"] += 1\n",
    "                return \"executed\"\n",
    "            execution_stats[\"statements_previewed\"] += 1\n",
    "            return \"previewed\"\n",
    "\n",
    "        execution_stats[\"statements_failed\"] += 1\n",
    "        raise RuntimeError(f\"Failed in {file_name} statement #{index}: {exc}\") from exc\n",
    "\n",
    "\n",
    "def execute_sql_file(path: Path) -> None:\n",
    "    if not path.exists() or not path.is_file():\n",
    "        raise FileNotFoundError(f\"SQL file not found: {path}\")\n",
    "\n",
    "    rendered = render_tokens(path.read_text(encoding=\"utf-8\"))\n",
    "    statements = split_sql_statements(rendered)\n",
    "    print(f\"Executing {path.name}: statements={len(statements)}\")\n",
    "\n",
    "    file_counts = {\"executed\": 0, \"previewed\": 0, \"skipped\": 0}\n",
    "    for idx, statement in enumerate(statements, start=1):\n",
    "        status = execute_sql_statement(statement, file_name=path.name, index=idx)\n",
    "        file_counts[status] = file_counts.get(status, 0) + 1\n",
    "\n",
    "    execution_stats[\"files\"].append(\n",
    "        {\n",
    "            \"name\": path.name,\n",
    "            \"statements\": len(statements),\n",
    "            \"executed\": file_counts.get(\"executed\", 0),\n",
    "            \"previewed\": file_counts.get(\"previewed\", 0),\n",
    "            \"skipped\": file_counts.get(\"skipped\", 0),\n",
    "        }\n",
    "    )\n",
    "    print(\n",
    "        f\"Completed {path.name}: executed={file_counts.get('executed', 0)} \"\n",
    "        f\"previewed={file_counts.get('previewed', 0)} skipped={file_counts.get('skipped', 0)}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prod_push_code_03",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "OBJECT_SQL_FILES = [\n",
    "    \"00_create_v1_schema.sql\",\n",
    "    \"01_create_lookup_tables.sql\",\n",
    "    \"02_create_core_tables.sql\",\n",
    "    \"03_create_assignment_tables.sql\",\n",
    "    \"04_create_governance_tables.sql\",\n",
    "    \"05_create_functional_parity_bridge.sql\",\n",
    "    \"06_create_functional_runtime_compat.sql\",\n",
    "    \"07_create_reporting_views.sql\",\n",
    "    \"90_create_indexes.sql\",\n",
    "]\n",
    "\n",
    "SEED_SQL_FILES = [\n",
    "    \"94_seed_critical_reference_data.sql\",\n",
    "    \"95_seed_base_security_roles.sql\",\n",
    "    \"96_seed_help_center.sql\",\n",
    "]\n",
    "\n",
    "if include_optimize:\n",
    "    object_sql_files_to_run = list(OBJECT_SQL_FILES)\n",
    "else:\n",
    "    object_sql_files_to_run = [name for name in OBJECT_SQL_FILES if name != \"90_create_indexes.sql\"]\n",
    "\n",
    "CREATE_TABLE_RE = re.compile(r\"(?is)CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?([`\\w\\.]+)\")\n",
    "CREATE_VIEW_RE = re.compile(r\"(?is)CREATE\\s+(?:OR\\s+REPLACE\\s+)?VIEW\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?([`\\w\\.]+)\")\n",
    "\n",
    "\n",
    "def preflight_sql_files() -> None:\n",
    "    sql_root_path = Path(sql_root)\n",
    "    if not sql_root_path.exists() or not sql_root_path.is_dir():\n",
    "        raise FileNotFoundError(\n",
    "            \"sql_root does not exist or is not a directory: \"\n",
    "            f\"{sql_root_path}. Set sql_root to your workspace folder ending in /setup/production_push/sql.\"\n",
    "        )\n",
    "\n",
    "    required_files = list(object_sql_files_to_run)\n",
    "    if include_seed:\n",
    "        required_files.extend(SEED_SQL_FILES)\n",
    "\n",
    "    missing = [name for name in required_files if not (sql_root_path / name).is_file()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing SQL files under {sql_root_path}: {', '.join(missing)}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Preflight SQL root OK: {sql_root_path}\")\n",
    "\n",
    "\n",
    "def expected_objects_from_sql() -> tuple[set[str], set[str]]:\n",
    "    expected_tables: set[str] = set()\n",
    "    expected_views: set[str] = set()\n",
    "\n",
    "    for file_name in OBJECT_SQL_FILES:\n",
    "        sql_file = Path(sql_root) / file_name\n",
    "        if not sql_file.exists() or not sql_file.is_file():\n",
    "            raise FileNotFoundError(f\"Required object SQL file not found: {sql_file}\")\n",
    "\n",
    "        rendered = render_tokens(sql_file.read_text(encoding=\"utf-8\"))\n",
    "        expected_tables.update(normalize_object_name(item) for item in CREATE_TABLE_RE.findall(rendered))\n",
    "        expected_views.update(normalize_object_name(item) for item in CREATE_VIEW_RE.findall(rendered))\n",
    "\n",
    "    return expected_tables, expected_views\n",
    "\n",
    "\n",
    "preflight_sql_files()\n",
    "expected_tables, expected_views = expected_objects_from_sql()\n",
    "print(f\"Expected tables: {len(expected_tables)}\")\n",
    "print(f\"Expected views: {len(expected_views)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prod_push_code_04",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def list_schema_objects() -> tuple[set[str], set[str]]:\n",
    "    query = (\n",
    "        f\"SELECT table_name, table_type \"\n",
    "        f\"FROM `{catalog}`.information_schema.tables \"\n",
    "        f\"WHERE table_schema = '{schema}'\"\n",
    "    )\n",
    "    rows = spark.sql(query).collect()\n",
    "\n",
    "    tables: set[str] = set()\n",
    "    views: set[str] = set()\n",
    "    for row in rows:\n",
    "        name = str(row[\"table_name\"] or \"\").strip().lower()\n",
    "        table_type = str(row[\"table_type\"] or \"\").strip().upper()\n",
    "        if not name:\n",
    "            continue\n",
    "        if table_type == \"VIEW\":\n",
    "            views.add(name)\n",
    "        else:\n",
    "            tables.add(name)\n",
    "\n",
    "    return tables, views\n",
    "\n",
    "\n",
    "def print_schema_counts(label: str) -> tuple[set[str], set[str]]:\n",
    "    tables, views = list_schema_objects()\n",
    "    print(f\"{label}: tables={len(tables)} views={len(views)}\")\n",
    "    return tables, views\n",
    "\n",
    "\n",
    "def parse_excludes(raw_value: str) -> set[str]:\n",
    "    return {\n",
    "        value.strip().lower()\n",
    "        for value in str(raw_value or \"\").split(\",\")\n",
    "        if value.strip()\n",
    "    }\n",
    "\n",
    "\n",
    "def preflight_apply_permissions() -> None:\n",
    "    if dry_run:\n",
    "        print(\"Skipping write-permission preflight because dry_run=true\")\n",
    "        return\n",
    "\n",
    "    probe_name = f\"_prod_push_perm_probe_{run_id}\"\n",
    "    probe_ref = f\"`{catalog}`.`{schema}`.`{probe_name}`\"\n",
    "\n",
    "    try:\n",
    "        spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "        spark.sql(f\"USE SCHEMA `{schema}`\")\n",
    "    except Exception as exc:\n",
    "        raise AssertionError(\n",
    "            f\"Cannot access target catalog/schema {catalog}.{schema}: {exc}\"\n",
    "        ) from exc\n",
    "\n",
    "    try:\n",
    "        spark.sql(f\"CREATE TABLE {probe_ref} (probe_id STRING) USING DELTA\")\n",
    "        spark.sql(f\"INSERT INTO {probe_ref} VALUES ('ok')\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {probe_ref}\")\n",
    "        print(\"Write-permission preflight passed (CREATE/INSERT/DROP).\")\n",
    "    except Exception as exc:\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {probe_ref}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise AssertionError(\n",
    "            \"Write-permission preflight failed. Confirm CREATE TABLE and INSERT privileges on \"\n",
    "            f\"{catalog}.{schema}. Original error: {exc}\"\n",
    "        ) from exc\n",
    "\n",
    "\n",
    "def drop_objects_except(exclude_set: set[str]) -> None:\n",
    "    before_tables, before_views = print_schema_counts(\"Before drop_except\")\n",
    "\n",
    "    for name in sorted(before_views):\n",
    "        if name in exclude_set:\n",
    "            print(f\"Skip view (excluded): {name}\")\n",
    "            continue\n",
    "        execute_sql(f\"DROP VIEW IF EXISTS `{catalog}`.`{schema}`.`{name}`\")\n",
    "\n",
    "    for name in sorted(before_tables):\n",
    "        if name in exclude_set:\n",
    "            print(f\"Skip table (excluded): {name}\")\n",
    "            continue\n",
    "        execute_sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{name}`\")\n",
    "\n",
    "    print_schema_counts(\"After drop_except\")\n",
    "\n",
    "\n",
    "def truncate_tables_except(exclude_set: set[str]) -> None:\n",
    "    before_tables, _ = print_schema_counts(\"Before truncate_except\")\n",
    "    for name in sorted(before_tables):\n",
    "        if name in exclude_set:\n",
    "            print(f\"Skip table (excluded): {name}\")\n",
    "            continue\n",
    "        execute_sql(f\"TRUNCATE TABLE `{catalog}`.`{schema}`.`{name}`\")\n",
    "    print_schema_counts(\"After truncate_except\")\n",
    "\n",
    "\n",
    "def validate_schema() -> None:\n",
    "    existing_tables, existing_views = list_schema_objects()\n",
    "    missing_tables = sorted(expected_tables - existing_tables)\n",
    "    missing_views = sorted(expected_views - existing_views)\n",
    "\n",
    "    if missing_tables or missing_views:\n",
    "        if missing_tables:\n",
    "            print(\"Missing tables:\")\n",
    "            for name in missing_tables:\n",
    "                print(f\"- {name}\")\n",
    "        if missing_views:\n",
    "            print(\"Missing views:\")\n",
    "            for name in missing_views:\n",
    "                print(f\"- {name}\")\n",
    "        raise AssertionError(\"Schema validation failed.\")\n",
    "\n",
    "    print(\"Schema validation passed.\")\n",
    "\n",
    "\n",
    "def validate_seed_rows() -> None:\n",
    "    if dry_run or not include_seed:\n",
    "        return\n",
    "\n",
    "    seed_expectations = {\n",
    "        \"app_lookup_option\": 1,\n",
    "        \"sec_role_definition\": 1,\n",
    "        \"vendor_help_article\": 1,\n",
    "    }\n",
    "    failures: list[str] = []\n",
    "    for table_name, min_rows in seed_expectations.items():\n",
    "        query = f\"SELECT COUNT(*) AS cnt FROM `{catalog}`.`{schema}`.`{table_name}`\"\n",
    "        count = int(spark.sql(query).collect()[0][\"cnt\"])\n",
    "        print(f\"Seed check {table_name}: rows={count}\")\n",
    "        if count < int(min_rows):\n",
    "            failures.append(f\"{table_name} has {count} rows (expected >= {min_rows})\")\n",
    "\n",
    "    if failures:\n",
    "        raise AssertionError(\"Seed validation failed: \" + \"; \".join(failures))\n",
    "\n",
    "\n",
    "def apply_bundle(*, verify_permissions: bool = True) -> None:\n",
    "    if verify_permissions:\n",
    "        preflight_apply_permissions()\n",
    "\n",
    "    before_tables, before_views = print_schema_counts(\"Before apply\")\n",
    "\n",
    "    for file_name in object_sql_files_to_run:\n",
    "        execute_sql_file(Path(sql_root) / file_name)\n",
    "\n",
    "    if include_seed:\n",
    "        for file_name in SEED_SQL_FILES:\n",
    "            execute_sql_file(Path(sql_root) / file_name)\n",
    "\n",
    "    after_tables, after_views = print_schema_counts(\"After apply (pre-validate)\")\n",
    "\n",
    "    if not dry_run and expected_tables and len(after_tables) == 0:\n",
    "        raise AssertionError(\n",
    "            \"Apply completed but schema is still empty. Check CREATE TABLE permissions, sql_root path, and operation mode.\"\n",
    "        )\n",
    "\n",
    "    validate_schema()\n",
    "    validate_seed_rows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prod_push_code_05",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "exclude_set = parse_excludes(exclude_objects)\n",
    "\n",
    "print(\"Execution summary:\")\n",
    "print(f\"- run_id: {run_id}\")\n",
    "print(f\"- operation: {operation}\")\n",
    "print(f\"- catalog.schema: {catalog}.{schema}\")\n",
    "print(f\"- sql_root: {sql_root}\")\n",
    "print(f\"- include_seed: {include_seed}\")\n",
    "print(f\"- include_optimize: {include_optimize}\")\n",
    "print(f\"- dry_run: {dry_run}\")\n",
    "print(f\"- confirm_destructive: {confirm_destructive}\")\n",
    "print(f\"- exclude_objects: {sorted(exclude_set)}\")\n",
    "\n",
    "if operation == \"apply\":\n",
    "    apply_bundle()\n",
    "elif operation == \"rebuild\":\n",
    "    preflight_apply_permissions()\n",
    "    drop_objects_except(exclude_set)\n",
    "    apply_bundle(verify_permissions=False)\n",
    "elif operation == \"drop_except\":\n",
    "    drop_objects_except(exclude_set)\n",
    "elif operation == \"truncate_except\":\n",
    "    truncate_tables_except(exclude_set)\n",
    "elif operation == \"validate\":\n",
    "    validate_schema()\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported operation: {operation}\")\n",
    "\n",
    "if execution_stats[\"files\"]:\n",
    "    print(\"File execution stats:\")\n",
    "    for item in execution_stats[\"files\"]:\n",
    "        print(\n",
    "            f\"- {item['name']}: statements={item['statements']} \"\n",
    "            f\"executed={item['executed']} previewed={item['previewed']} skipped={item['skipped']}\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    \"Statement totals: \"\n",
    "    f\"executed={execution_stats['statements_executed']} \"\n",
    "    f\"previewed={execution_stats['statements_previewed']} \"\n",
    "    f\"skipped={execution_stats['statements_skipped']} \"\n",
    "    f\"failed={execution_stats['statements_failed']} \"\n",
    "    f\"retries={execution_stats['retries']}\"\n",
    ")\n",
    "\n",
    "if not dry_run:\n",
    "    tables, views = list_schema_objects()\n",
    "    print(f\"Current schema object counts: tables={len(tables)} views={len(views)}\")\n",
    "\n",
    "print(\"Operation complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
